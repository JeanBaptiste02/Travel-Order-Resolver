{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mast\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn_crfsuite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CRF\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_classification_report, flat_f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins\n",
    "dataset_path = \"C:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/dataset/tokens/token.csv\"\n",
    "model_path = \"C:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/models/crf/\"\n",
    "\n",
    "# Chargement des données\n",
    "print(\"Loading dataset...\")\n",
    "def load_dataset_with_progress(dataset_path):\n",
    "    with tqdm(total=100, desc=\"Loading dataset\") as pbar:\n",
    "        dataset = pd.read_csv(dataset_path, delimiter=';', quotechar='\"', names=[\"text\", \"tokens\", \"ner_tags\", \"spacy_ner_tags\"])\n",
    "        pbar.update(100)\n",
    "    return dataset\n",
    "\n",
    "dataset = load_dataset_with_progress(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement des données\n",
    "def safe_eval(val):\n",
    "    try:\n",
    "        val = val.replace('\"\"', '\"').replace(\"'\", '\"')\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "def prepare_data(dataset):\n",
    "    sequences = []\n",
    "    for _, row in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "        tokens = safe_eval(row[\"tokens\"])\n",
    "        spacy_ner_tags = safe_eval(row[\"spacy_ner_tags\"]) if pd.notnull(row[\"spacy_ner_tags\"]) else None\n",
    "        if tokens is None:\n",
    "            continue\n",
    "        labels = [\"O\"] * len(tokens)\n",
    "        if spacy_ner_tags:\n",
    "            for tag in spacy_ner_tags:\n",
    "                start, end, label = tag['start'], tag['end'], tag['label']\n",
    "                for i, token in enumerate(tokens):\n",
    "                    if start <= sum(len(t) + 1 for t in tokens[:i]) < end:\n",
    "                        labels[i] = label\n",
    "        sequences.append((tokens, labels))\n",
    "    return sequences\n",
    "\n",
    "print(\"Preparing data...\")\n",
    "data = prepare_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caractéristiques pour CRF\n",
    "def word2features(sent, i):\n",
    "    word = sent[i]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'BOS': i == 0,\n",
    "        'EOS': i == len(sent) - 1\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i - 1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent) - 1:\n",
    "        word1 = sent[i + 1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(labels):\n",
    "    return labels\n",
    "\n",
    "# Séparation des données\n",
    "print(\"Splitting data...\")\n",
    "train, test_valid = train_test_split(data, test_size=0.5, random_state=42)\n",
    "test, valid = train_test_split(test_valid, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train = [sent2features(s[0]) for s in train]\n",
    "y_train = [sent2labels(s[1]) for s in train]\n",
    "X_valid = [sent2features(s[0]) for s in valid]\n",
    "y_valid = [sent2labels(s[1]) for s in valid]\n",
    "X_test = [sent2features(s[0]) for s in test]\n",
    "y_test = [sent2labels(s[1]) for s in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle CRF\n",
    "print(\"Training CRF model...\")\n",
    "crf = CRF(algorithm='lbfgs', max_iterations=100, c1=0.1, c2=0.1, all_possible_transitions=True)\n",
    "\n",
    "losses = []\n",
    "for epoch in tqdm(range(20), desc=\"Training epochs\"):\n",
    "    crf.fit(X_train, y_train)\n",
    "    y_pred = crf.predict(X_valid)\n",
    "    f1 = flat_f1_score(y_valid, y_pred, average='weighted')\n",
    "    print(f\"Epoch {epoch + 1}: F1-Score on validation set: {f1:.4f}\")\n",
    "    losses.append(1 - f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation finale\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "y_pred = crf.predict(X_test)\n",
    "report = flat_classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du modèle\n",
    "import joblib\n",
    "print(\"Saving model...\")\n",
    "joblib.dump(crf, model_path + \"crf_model.joblib\")\n",
    "\n",
    "# Visualisation des pertes\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss per epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "# Test du modèle sur des phrases\n",
    "test_sentences = [\n",
    "    \"Je pars de Paris et j'arrive à Marseille.\",\n",
    "    \"Je vais à Bordeaux en partant de Toulouse.\",\n",
    "    \"Mon trajet va de VILLIERS SUR LOIR à JARNY.\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting on new sentences...\")\n",
    "for sentence in test_sentences:\n",
    "    tokens = sentence.split()\n",
    "    features = sent2features(tokens)\n",
    "    prediction = crf.predict([features])[0]\n",
    "    print(f\"\\nPhrase: {sentence}\")\n",
    "    for token, label in zip(tokens, prediction):\n",
    "        print(f\" - {token}: {label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travelorder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
