{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForTokenClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths defined\n"
     ]
    }
   ],
   "source": [
    "model_path = \"C:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/models/distilbert_latest/\"\n",
    "\n",
    "print(\"paths defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading DistilBERT tokenizer:   0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\vikne\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading DistilBERT tokenizer: 100%|██████████| 100/100 [00:01<00:00, 84.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DistilBERT tokenizer loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Chargement du tokenizer DistilBERT\n",
    "def load_tokenizer_with_progress(model_name):\n",
    "    with tqdm(total=100, desc=\"Loading DistilBERT tokenizer\") as pbar:\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        pbar.update(100)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "tokenizer = load_tokenizer_with_progress(\"distilbert-base-uncased\")\n",
    "print(\"\\nDistilBERT tokenizer loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DistilBERT model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Chargement du modèle DistilBERT pour la classification par tokens\n",
    "model = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "print(\"\\nDistilBERT model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 1/4: C:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/dataset/token_classification/dataset_token_classification_1.csv\n",
      "Loading dataset 2/4: C:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/dataset/token_classification/dataset_token_classification_1.csv\n",
      "Loading dataset 3/4: C:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/dataset/token_classification/dataset_token_classification_1.csv\n",
      "Loading dataset 4/4: C:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/dataset/token_classification/dataset_token_classification_1.csv\n",
      "Total dataset loaded successfully: 435716 rows\n"
     ]
    }
   ],
   "source": [
    "# Chargement des données\n",
    "def load_multiple_datasets(dataset_paths):\n",
    "    dataframes = []\n",
    "    for i, path in enumerate(dataset_paths):\n",
    "        print(f\"Loading dataset {i+1}/{len(dataset_paths)}: {path}\")\n",
    "        dataset = pd.read_csv(\n",
    "            path,\n",
    "            delimiter=';',\n",
    "            quotechar='\"',\n",
    "            names=[\"text\", \"tokens\", \"ner_tags\", \"spacy_ner_tags\"]\n",
    "        )\n",
    "        dataframes.append(dataset)\n",
    "\n",
    "    combined_dataset = pd.concat(dataframes, ignore_index=True)\n",
    "    return combined_dataset\n",
    "\n",
    "\n",
    "dataset_paths = [\n",
    "    \"C:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/dataset/token_classification/dataset_token_classification_1.csv\",\n",
    "    \"C:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/dataset/token_classification/dataset_token_classification_1.csv\",\n",
    "    \"C:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/dataset/token_classification/dataset_token_classification_1.csv\",\n",
    "    \"C:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/dataset/token_classification/dataset_token_classification_1.csv\"\n",
    "]\n",
    "\n",
    "dataset = load_multiple_datasets(dataset_paths)\n",
    "print(f\"Total dataset loaded successfully: {len(dataset)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation et tokenisation avec DistilBERT\n",
    "def safe_eval(val):\n",
    "    try:\n",
    "        val = val.replace('\"\"', '\"').replace(\"'\", '\"')\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_entity_well_aligned(text, start, end):\n",
    "    if start > 0 and text[start - 1].isalnum():\n",
    "        return False\n",
    "    if end < len(text) and text[end].isalnum():\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 435716/435716 [02:38<00:00, 2752.79it/s]\n",
      "100%|██████████| 334704/334704 [13:01<00:00, 428.24it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Préparation des données d'entraînement\n",
    "print(\"Preparing training data...\")\n",
    "CONVERTED_TRAIN_DATA = []\n",
    "\n",
    "for _, item in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "    tokens_str = item['tokens'].replace(\"'\", '\"')\n",
    "    spacy_ner_tags_str = item['spacy_ner_tags'].replace(\"'\", '\"') if pd.notnull(item['spacy_ner_tags']) else None\n",
    "\n",
    "    tokens = safe_eval(tokens_str)\n",
    "    if tokens is None:\n",
    "        continue\n",
    "\n",
    "    # Créer la liste de labels avec une taille correspondant à celle des tokens\n",
    "    labels = [0] * len(tokens)  # Par défaut, toutes les entités sont non-entities\n",
    "\n",
    "    if spacy_ner_tags_str:\n",
    "        annotations = safe_eval(spacy_ner_tags_str)\n",
    "        if annotations:\n",
    "            for annotation in annotations:\n",
    "                # Vérifier la validité des indices\n",
    "                start, end, label = annotation['start'], annotation['end'], annotation['label']\n",
    "                \n",
    "                # Token alignement - Assurez-vous que les tokens sont correctement indexés\n",
    "                token_start = 0\n",
    "                for idx, token in enumerate(tokens):\n",
    "                    token_end = token_start + len(token)  # Déterminer la fin de chaque token\n",
    "                    if token_start <= start < token_end:\n",
    "                        # L'annotation commence dans ce token\n",
    "                        labels[idx] = 1\n",
    "                    token_start = token_end + 1  # Passer au token suivant\n",
    "\n",
    "    CONVERTED_TRAIN_DATA.append({\n",
    "        \"tokens\": tokens,\n",
    "        \"labels\": labels\n",
    "    })\n",
    "\n",
    "# Tokeniser les données\n",
    "tokenized_texts = []\n",
    "tokenized_labels = []\n",
    "\n",
    "for item in tqdm(CONVERTED_TRAIN_DATA):\n",
    "    encoding = tokenizer(item['tokens'], is_split_into_words=True, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\")\n",
    "    tokenized_texts.append(encoding)\n",
    "    # Ajuster les labels en fonction de la tokenisation\n",
    "    labels_padded = item['labels'][:64] + [0] * (64 - len(item['labels'][:64]))\n",
    "    tokenized_labels.append(torch.tensor(labels_padded, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 167352 examples\n",
      "Test data: 167352 examples\n",
      "Validation data: 83676 examples\n"
     ]
    }
   ],
   "source": [
    "# Séparation en ensembles d'entraînement, test et validation\n",
    "train_texts, test_valid_texts = train_test_split(tokenized_texts, test_size=0.5, random_state=42)\n",
    "valid_texts, test_texts = train_test_split(test_valid_texts, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train data: {len(train_texts)} examples\")\n",
    "print(f\"Test data: {len(train_texts)} examples\")\n",
    "print(f\"Validation data: {len(valid_texts)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikne\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\vikne\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\vikne\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'tarfile' from 'backports' (C:\\Users\\vikne\\anaconda3\\Lib\\site-packages\\backports\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Créer un Optimizer personnalisé\u001b[39;00m\n\u001b[0;32m     31\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:500\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    498\u001b[0m default_callbacks \u001b[38;5;241m=\u001b[39m DEFAULT_CALLBACKS \u001b[38;5;241m+\u001b[39m get_reporting_integration_callbacks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mreport_to)\n\u001b[0;32m    499\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m default_callbacks \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m default_callbacks \u001b[38;5;241m+\u001b[39m callbacks\n\u001b[1;32m--> 500\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler \u001b[38;5;241m=\u001b[39m \u001b[43mCallbackHandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_callback(PrinterCallback \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdisable_tqdm \u001b[38;5;28;01melse\u001b[39;00m DEFAULT_PROGRESS_CALLBACK)\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# Will be set to True by `self._setup_loggers()` on first call to `self.log()`.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer_callback.py:313\u001b[0m, in \u001b[0;36mCallbackHandler.__init__\u001b[1;34m(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks:\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer_callback.py:330\u001b[0m, in \u001b[0;36mCallbackHandler.add_callback\u001b[1;34m(self, callback)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_callback\u001b[39m(\u001b[38;5;28mself\u001b[39m, callback):\n\u001b[1;32m--> 330\u001b[0m     cb \u001b[38;5;241m=\u001b[39m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callback, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m callback\n\u001b[0;32m    331\u001b[0m     cb_class \u001b[38;5;241m=\u001b[39m callback \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callback, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m callback\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cb_class \u001b[38;5;129;01min\u001b[39;00m [c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks]:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:1409\u001b[0m, in \u001b[0;36mCodeCarbonCallback.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_codecarbon_available():\n\u001b[0;32m   1406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCodeCarbonCallback requires `codecarbon` to be installed. Run `pip install codecarbon`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1408\u001b[0m     )\n\u001b[1;32m-> 1409\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcodecarbon\u001b[39;00m\n\u001b[0;32m   1411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_codecarbon \u001b[38;5;241m=\u001b[39m codecarbon\n\u001b[0;32m   1412\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\codecarbon\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThe Carbon Tracker module. The following objects/decorators belong to the Public API\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01memissions_tracker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     EmissionsTracker,\n\u001b[0;32m      8\u001b[0m     OfflineEmissionsTracker,\n\u001b[0;32m      9\u001b[0m     track_emissions,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmissionsTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOfflineEmissionsTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack_emissions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\codecarbon\\emissions_tracker.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, List, Optional, Union\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecarbon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecarbon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cpu, gpu\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecarbon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_hierarchical_config, parse_gpu_ids\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecarbon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01memissions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Emissions\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\codecarbon\\core\\cpu.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecarbon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect_cpu_model\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecarbon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecarbon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataSource\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_powergadget_available\u001b[39m():\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\codecarbon\\input.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpkg_resources\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDataSource\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pkg_resources\\__init__.py:90\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjaraco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drop_comment, join_continuation, yield_lines\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplatformdirs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m user_cache_dir \u001b[38;5;28;01mas\u001b[39;00m _user_cache_dir\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\jaraco\\text\\__init__.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib_resources\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjaraco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compose, method_cache\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjaraco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExceptionTrap\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubstitution\u001b[39m(old, new):\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    Return a function that will perform a substitution on a string\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\jaraco\\context.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Iterator\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m12\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbackports\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tarfile\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtarfile\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'tarfile' from 'backports' (C:\\Users\\vikne\\anaconda3\\Lib\\site-packages\\backports\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer, AdamW\n",
    "\n",
    "\n",
    "# Fonction pour les métriques\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = torch.argmax(torch.tensor(predictions), dim=-1).numpy()\n",
    "    precision = precision_score(labels, predictions, zero_division=1)\n",
    "    recall = recall_score(labels, predictions, zero_division=1)\n",
    "    f1 = f1_score(labels, predictions, zero_division=1)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "# Initialiser le tokenizer et le modèle\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Configurer l'entraînement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/chemin/vers/mon/dossier\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Créer un Optimizer personnalisé\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_texts,\n",
    "    eval_dataset=valid_texts,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, None),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du modèle\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "def draw_scores(metrics_hist):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    sns.lineplot(x=list(range(len(metrics_hist))), y=metrics_hist, ax=axs[0, 0])\n",
    "    axs[0, 0].set_title('Precision, Recall, F1')\n",
    "\n",
    "draw_scores([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du modèle\n",
    "test_sentences = [\n",
    "    \"Je pars de Paris et j'arrive à Marseille.\",\n",
    "    \"Je vais à Bordeaux en partant de Toulouse.\"\n",
    "]\n",
    "\n",
    "def test_model(model, tokenizer, sentences):\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            predictions = model(**tokens)\n",
    "        print(predictions)\n",
    "\n",
    "test_model(model, tokenizer, test_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travelorder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
