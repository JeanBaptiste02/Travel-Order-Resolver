{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported\n",
      "Paths defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64000/64000 [1:36:42<00:00, 11.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: -101676.2487624628, F1: 0.06, Precision: 0.06, Recall: 0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 62435/64000 [2:09:15<08:24,  3.10it/s]  "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "print(\"Libraries imported\")\n",
    "\n",
    "# Chemins\n",
    "dataset_path = \"C:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/dataset/tokens/token.csv\"\n",
    "model_path = \"C:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/models/bilstmcrf/\"\n",
    "\n",
    "print(\"Paths defined\")\n",
    "\n",
    "# Définition de la CRF avec PyTorch pur\n",
    "class CRF(nn.Module):\n",
    "    def __init__(self, num_tags, batch_first=True):\n",
    "        super(CRF, self).__init__()\n",
    "        self.num_tags = num_tags\n",
    "        self.batch_first = batch_first\n",
    "        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))\n",
    "\n",
    "    def forward(self, emissions, tags, mask):\n",
    "        return self._calculate_score(emissions, tags, mask)\n",
    "\n",
    "    def _calculate_score(self, emissions, tags, mask):\n",
    "        batch_size, seq_len, num_tags = emissions.size()\n",
    "        score = torch.zeros(batch_size, device=emissions.device)\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            active_mask = mask[:, t]\n",
    "            if t > 0:\n",
    "                score += self.transitions[tags[:, t - 1], tags[:, t]] * active_mask\n",
    "            score += emissions[torch.arange(batch_size), t, tags[:, t]] * active_mask  # Corrected this line\n",
    "\n",
    "        return score.sum()\n",
    "\n",
    "\n",
    "    def decode(self, emissions, mask):\n",
    "        batch_size, seq_len, num_tags = emissions.size()\n",
    "\n",
    "        # Initialisation\n",
    "        viterbi = emissions[:, 0]  # Taille (batch_size, num_tags)\n",
    "        backpointers = []\n",
    "\n",
    "        for t in range(1, seq_len):\n",
    "            broadcast_viterbi = viterbi.unsqueeze(2)  # Taille (batch_size, num_tags, 1)\n",
    "            broadcast_emissions = emissions[:, t].unsqueeze(1)  # Taille (batch_size, 1, num_tags)\n",
    "\n",
    "            # Calcul des scores\n",
    "            score = broadcast_viterbi + self.transitions + broadcast_emissions\n",
    "            viterbi, backpointer = torch.max(score, dim=1)  # Taille (batch_size, num_tags)\n",
    "            backpointers.append(backpointer)\n",
    "\n",
    "        # Récupération des meilleures séquences\n",
    "        best_tags_list = []\n",
    "        for i in range(batch_size):\n",
    "            best_tag = torch.argmax(viterbi[i]).item()\n",
    "            best_tags = [best_tag]\n",
    "            for backpointer in reversed(backpointers):\n",
    "                best_tag = backpointer[i][best_tag].item()\n",
    "                best_tags.append(best_tag)\n",
    "            best_tags.reverse()\n",
    "            best_tags_list.append(best_tags)\n",
    "\n",
    "        return best_tags_list\n",
    "\n",
    "\n",
    "# Modèle BiLSTM-CRF\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_tags, padding_idx):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2, num_tags)\n",
    "        self.crf = CRF(num_tags)\n",
    "    \n",
    "    def forward(self, sentences, tags, mask):\n",
    "        embeddings = self.embedding(sentences)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        return self.crf(emissions, tags, mask)\n",
    "\n",
    "    def predict(self, sentences, mask):\n",
    "        embeddings = self.embedding(sentences)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        emissions = self.hidden2tag(lstm_out)  # Taille (batch_size, seq_len, num_tags)\n",
    "        return self.crf.decode(emissions, mask)\n",
    "\n",
    "# Charger et prétraiter les données\n",
    "def load_data(dataset_path, sample_size=80000):\n",
    "    df = pd.read_csv(dataset_path, sep=\";\")\n",
    "    if len(df) > sample_size:\n",
    "        df = df.sample(n=sample_size, random_state=42)  # Sélection aléatoire de 80 000 lignes\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        tokens = ast.literal_eval(row['tokens'])\n",
    "        ner_tags = ast.literal_eval(row['ner_tags'])\n",
    "        sentences.append(tokens)\n",
    "        labels.append(ner_tags)\n",
    "    return sentences, labels\n",
    "\n",
    "# Charger les données avec échantillonnage\n",
    "sentences, labels = load_data(dataset_path, sample_size=80000)\n",
    "\n",
    "# Création du vocabulaire\n",
    "word_to_idx = {\"<PAD>\": 0}\n",
    "tag_to_idx = {}\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "for label_seq in labels:\n",
    "    for label in label_seq:\n",
    "        if label not in tag_to_idx:\n",
    "            tag_to_idx[label] = len(tag_to_idx)\n",
    "\n",
    "# Conversion des données\n",
    "def prepare_data(sentences, labels, word_to_idx, tag_to_idx):\n",
    "    X_data = [[word_to_idx[word] for word in sentence] for sentence in sentences]\n",
    "    y_data = [[tag_to_idx[label] for label in label_seq] for label_seq in labels]\n",
    "    return X_data, y_data\n",
    "\n",
    "X_data, y_data = prepare_data(sentences, labels, word_to_idx, tag_to_idx)\n",
    "\n",
    "# Division des données\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paramètres du modèle\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "num_tags = len(tag_to_idx)\n",
    "padding_idx = word_to_idx[\"<PAD>\"]\n",
    "\n",
    "model = BiLSTM_CRF(len(word_to_idx), embedding_dim, hidden_dim, num_tags, padding_idx)\n",
    "\n",
    "# Optimiseur et fonction de perte\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entraînement du modèle avec suivi des scores\n",
    "losses_hist, f1_hist, recall_hist, precision_hist = [], [], [], []\n",
    "\n",
    "def calculate_scores(y_true, y_pred):\n",
    "    tp = sum([1 for yt, yp in zip(y_true, y_pred) if yt == yp])\n",
    "    fn = len(y_true) - tp\n",
    "    fp = len(y_pred) - tp\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    return precision, recall, f1\n",
    "\n",
    "def train(model, X_train, y_train, optimizer, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        y_true_all, y_pred_all = [], []\n",
    "        for i in tqdm(range(len(X_train))):\n",
    "            sentence = torch.tensor(X_train[i], dtype=torch.long).unsqueeze(0)\n",
    "            tags = torch.tensor(y_train[i], dtype=torch.long).unsqueeze(0)\n",
    "            mask = (sentence != padding_idx).long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(sentence, tags, mask)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                predicted_tags = model.predict(sentence, mask)\n",
    "                y_true_all.extend(y_train[i])\n",
    "                y_pred_all.extend(predicted_tags[0])  # Ajout [0] pour le batch unique\n",
    "\n",
    "        precision, recall, f1 = calculate_scores(y_true_all, y_pred_all)\n",
    "        losses_hist.append(total_loss / len(X_train))\n",
    "        f1_hist.append(f1)\n",
    "        recall_hist.append(recall)\n",
    "        precision_hist.append(precision)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(X_train)}, F1: {f1:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}')\n",
    "\n",
    "# Entraîner le modèle\n",
    "train(model, X_train, y_train, optimizer, num_epochs=5)\n",
    "\n",
    "# Dessiner les graphiques\n",
    "def draw_scores(losses_hist, f1_hist, recall_hist, precision_hist):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    axs[0, 0].plot(losses_hist, label=\"Loss\")\n",
    "    axs[0, 1].plot(f1_hist, label=\"F1 Score\", color=\"orange\")\n",
    "    axs[1, 0].plot(recall_hist, label=\"Recall\", color=\"green\")\n",
    "    axs[1, 1].plot(precision_hist, label=\"Precision\", color=\"blue\")\n",
    "    for ax in axs.flat:\n",
    "        ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "draw_scores(losses_hist, f1_hist, recall_hist, precision_hist)\n",
    "\n",
    "# Tester le modèle\n",
    "def test(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in tqdm(range(len(X_test))):\n",
    "        sentence = torch.tensor(X_test[i], dtype=torch.long).unsqueeze(0)\n",
    "        tags = torch.tensor(y_test[i], dtype=torch.long).unsqueeze(0)\n",
    "        mask = (sentence != padding_idx).long()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predicted_tags = model.predict(sentence, mask)\n",
    "        \n",
    "        correct += sum(yt == yp for yt, yp in zip(y_test[i], predicted_tags))\n",
    "        total += len(y_test[i])\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Tester le modèle\n",
    "test(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet_ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
