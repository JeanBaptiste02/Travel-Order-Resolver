{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Layer\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(\"Bibliothèques importées avec succès.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"C:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/dataset/text/text_intention_detector.csv\"\n",
    "\n",
    "def load_dataset():\n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        print(\"Chargement des données...\")\n",
    "        data = pd.read_csv(DATASET_PATH, delimiter=';')\n",
    "        print(\"Données chargées avec succès.\")\n",
    "        return data\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Le chemin du dataset est incorrect.\")\n",
    "\n",
    "data = load_dataset()\n",
    "\n",
    "print(\"\\nLes premières lignes du jeu de données :\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    label_mapping = {'is_correct': 0, 'is_not_trip': 1, 'is_unknown': 2}\n",
    "    data['label'] = data[['is_correct', 'is_not_trip', 'is_unknown']].idxmax(axis=1).map(label_mapping)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['sentence'], data['label'], test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train_raw, X_val_raw, X_test_raw, y_train, y_val, y_test = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "max_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_raw)\n",
    "\n",
    "X_train_padded = pad_sequences(tokenizer.texts_to_sequences(X_train_raw), maxlen=max_length, padding='post')\n",
    "X_val_padded = pad_sequences(tokenizer.texts_to_sequences(X_val_raw), maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(tokenizer.texts_to_sequences(X_test_raw), maxlen=max_length, padding='post')\n",
    "\n",
    "print(\"Tokenisation et padding terminés.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.norm = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = Dropout(0.1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.attention(inputs, inputs)\n",
    "        out = self.norm(inputs + self.dropout(attn_output))\n",
    "        return out\n",
    "\n",
    "def create_self_attention_model(vocab_size, embed_dim, max_length, num_labels, num_heads):\n",
    "    inputs = Input(shape=(max_length,))\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=max_length)(inputs)\n",
    "    x = SelfAttentionLayer(embed_dim, num_heads)(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(num_labels, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=5e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "num_labels = 3\n",
    "\n",
    "model = create_self_attention_model(vocab_size, embed_dim, max_length, num_labels, num_heads)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "print(\"Début de l'entraînement...\")\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    validation_data=(X_val_padded, y_val),\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Entraînement terminé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nÉvaluation sur l'ensemble de test...\")\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f\"Perte: {loss:.4f}, Précision: {accuracy:.4f}\")\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_test_padded), axis=1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap=\"Blues\")\n",
    "    plt.title(\"Matrice de Confusion\")\n",
    "    plt.xlabel(\"Prédictions\")\n",
    "    plt.ylabel(\"Valeurs Réelles\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(cm, labels=[\"is_correct\", \"is_not_trip\", \"is_unknown\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_texts(model, tokenizer, new_texts, max_length):\n",
    "    sequences = tokenizer.texts_to_sequences(new_texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    predictions = model.predict(padded_sequences)\n",
    "\n",
    "    for i, text in enumerate(new_texts):\n",
    "        print(f\"\\nTexte: {text}\")\n",
    "        for j, label in enumerate([\"is_correct\", \"is_not_trip\", \"is_unknown\"]):\n",
    "            print(f\" - {label}: {round(predictions[i][j] * 100, 2)}%\")\n",
    "\n",
    "new_texts = [\n",
    "    \"Je veux aller de Port-Boulet à Le Havre.\",\n",
    "    \"Je veux aller de Nantes à Nantes.\",\n",
    "    \"Comment aller à Niort depuis Troyes ?\"\n",
    "]\n",
    "predict_new_texts(model, tokenizer, new_texts, max_length)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
