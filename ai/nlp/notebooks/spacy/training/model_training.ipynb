{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6769e4b",
   "metadata": {},
   "source": [
    "# Description du Notebook\n",
    "\n",
    "Ce notebook permet d'entraîner un modèle de NER en français avec spaC en se basant les infos de voyage. \n",
    "\n",
    "Les étapes incluent : \n",
    "- l'importation, le nettoyage et la préparation des données annotées,\n",
    "- l'entraînement du modèle. \n",
    "\n",
    "Ensuite, on fait un comparaison de performances entre le modèle personnalisé et le modèle spaCy préentraîné, avec des visualisations pour évaluer la diversité et le nombre d'entités détectées par chaque modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aea1e95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 14, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Example\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Charger les jeux de données\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m text_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/dataset/raw/generated_dataset/text.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m token_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/dataset/raw/generated_dataset/token.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinished\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1749\u001b[0m         nrows\n\u001b[0;32m   1750\u001b[0m     )\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 14, saw 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import minibatch\n",
    "from spacy.training.example import Example\n",
    "\n",
    "# Charger les jeux de données\n",
    "text_data = pd.read_csv('C:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/dataset/raw/generated_dataset/text.csv')\n",
    "token_data = pd.read_csv('C:/Users/vikne/Documents/Master 2/Semestre 9/Intelligence artificielle/Travel-Order-Resolver/ai/nlp/dataset/raw/generated_dataset/token.csv')\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a7a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ner_data(df):\n",
    "    nlp_blank = spacy.blank(\"fr\")  # Modèle vierge pour le français\n",
    "    doc_bin = DocBin()\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        text = row['text']  # Colonne contenant les phrases\n",
    "        entities = row['entities']  # Colonne avec les entités (ex: [(start, end, label)])\n",
    "        \n",
    "        doc = nlp_blank.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in entities:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is not None:\n",
    "                ents.append(span)\n",
    "        \n",
    "        doc.ents = ents\n",
    "        doc_bin.add(doc)\n",
    "    \n",
    "    return doc_bin\n",
    "\n",
    "# Préparer les données et les sauvegarder pour l’entraînement\n",
    "doc_bin = prepare_ner_data(token_data)\n",
    "doc_bin.to_disk(\"ner_training.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed183f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un modèle vierge SpaCy\n",
    "nlp = spacy.blank(\"fr\")\n",
    "\n",
    "# Configurer le pipeline NER\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Ajouter les labels trouvés dans les données\n",
    "for _, row in token_data.iterrows():\n",
    "    for _, _, label in row['entities']:\n",
    "        ner.add_label(label)\n",
    "\n",
    "# Charger les données d’entraînement\n",
    "doc_bin = DocBin().from_disk(\"ner_training.spacy\")\n",
    "train_data = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "# Entraînement du modèle\n",
    "optimizer = nlp.begin_training()\n",
    "for epoch in range(10):\n",
    "    losses = {}\n",
    "    batches = minibatch(train_data, size=8)\n",
    "    for batch in batches:\n",
    "        examples = [Example.from_dict(nlp.make_doc(doc.text), {\"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]}) for doc in batch]\n",
    "        nlp.update(examples, drop=0.3, losses=losses, sgd=optimizer)\n",
    "    print(f\"Epoch {epoch}, Losses: {losses}\")\n",
    "\n",
    "# Sauvegarde du modèle entraîné\n",
    "nlp.to_disk(\"nlp/models/spacy/ner_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6488015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_classification_data(df):\n",
    "    texts = df['text'].tolist()\n",
    "    labels = df['label'].tolist()  # On suppose qu'il y a une colonne \"label\" avec les intentions\n",
    "    \n",
    "    return [(text, label) for text, label in zip(texts, labels)]\n",
    "\n",
    "# Préparer les données\n",
    "train_data = prepare_text_classification_data(text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline.textcat import Config, SingleLabelCategorizer\n",
    "\n",
    "# Charger un modèle vierge\n",
    "nlp = spacy.blank(\"fr\")\n",
    "\n",
    "# Configurer le pipeline de Text Classification\n",
    "textcat = nlp.add_pipe(\"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"})\n",
    "\n",
    "# Ajouter les étiquettes de classes\n",
    "for label in set(text_data['label']):\n",
    "    textcat.add_label(label)\n",
    "\n",
    "# Préparation des données d'entraînement\n",
    "train_data = [(nlp.make_doc(text), {\"cats\": {label: label == label}}) for text, label in train_data]\n",
    "\n",
    "# Entraînement du modèle\n",
    "optimizer = nlp.begin_training()\n",
    "for epoch in range(10):\n",
    "    losses = {}\n",
    "    batches = minibatch(train_data, size=8)\n",
    "    for batch in batches:\n",
    "        nlp.update(batch, sgd=optimizer, losses=losses)\n",
    "    print(f\"Epoch {epoch}, Losses: {losses}\")\n",
    "\n",
    "# Sauvegarder le modèle entraîné\n",
    "nlp.to_disk(\"nlp/models/spacy/textcat_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a56848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_ner(nlp, examples):\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    for example in examples:\n",
    "        doc = nlp(example.text)\n",
    "        true_labels.extend([ent.label_ for ent in example.ents])\n",
    "        pred_labels.extend([ent.label_ for ent in doc.ents])\n",
    "    \n",
    "    print(classification_report(true_labels, pred_labels))\n",
    "\n",
    "# Charger les données de test et le modèle NER\n",
    "nlp_ner = spacy.load(\"nlp/models/spacy/ner_model\")\n",
    "evaluate_ner(nlp_ner, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fc31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les modèles\n",
    "nlp_ner = spacy.load(\"nlp/models/spacy/ner_model\")\n",
    "nlp_textcat = spacy.load(\"nlp/models/spacy/textcat_model\")\n",
    "\n",
    "def process_request(text):\n",
    "    # Étape 1 : Détection de l'intention\n",
    "    doc = nlp_textcat(text)\n",
    "    intent = max(doc.cats, key=doc.cats.get)\n",
    "    \n",
    "    # Étape 2 : Extraction des entités\n",
    "    doc = nlp_ner(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    return intent, entities\n",
    "\n",
    "# Exemple\n",
    "text = \"Je veux aller de Nantes à Marseille\"\n",
    "intent, entities = process_request(text)\n",
    "print(f\"Intention : {intent}\")\n",
    "print(f\"Entités : {entities}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
