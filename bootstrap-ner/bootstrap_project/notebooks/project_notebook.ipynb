{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6726b4ca-19ac-44e1-aea6-172bcbc11081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from transformers import CamembertTokenizer, CamembertForTokenClassification\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import sentencepiece\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8762cf66-a302-466f-b550-60f645d30509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vérification GPU...\n",
      "Aucun GPU détecté, l'entraînement se fera sur le CPU\n"
     ]
    }
   ],
   "source": [
    "print(\"Vérification GPU...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # cuda : Compute Unified Device Architecture\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU détecté\")\n",
    "else:\n",
    "    print(\"Aucun GPU détecté, l'entraînement se fera sur le CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "389047d8-7a88-4050-98a3-328d41f5e13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ner_dataset file\n",
      "\n",
      "Premières lignes du dataset ner_dataset.csv :\n",
      "\n",
      "    Sentence #           Word  POS Tag\n",
      "0  Sentence: 1      Thousands  NNS   O\n",
      "1          NaN             of   IN   O\n",
      "2          NaN  demonstrators  NNS   O\n",
      "3          NaN           have  VBP   O\n",
      "4          NaN        marched  VBN   O\n",
      "\n",
      "######################################################################\n",
      "\n",
      "Loaded bottins file \n",
      "\n",
      "Premières lignes du dataset bottins.csv :\n",
      "\n",
      "                                                Text           Source\n",
      "0  <PER>Dufan et Clémendot</PER>, <ACT>pharmacien...   \"Bottin1_1820\"\n",
      "1  <PER>Dufant (Victor)</PER>, <ACT>libraire</ACT...   \"Bottin1_1820\"\n",
      "2  <PER>Dufay</PER>, <ACT>essayeur du commerce</A...   \"Bottin1_1820\"\n",
      "3  <PER>Dulay</PER>, <ACT>chandronnier</ACT>, <LO...   \"Bottin1_1820\"\n",
      "4  <PER>Dufay (V.e)</PER>, <ACT>grenetière</ACT>,...   \"Bottin1_1820\"\n"
     ]
    }
   ],
   "source": [
    "def load_ner_dataset_file():\n",
    "    ner_data = pd.read_csv(\"../../../../students_bootstrap/corpus/ner_dataset.csv\", encoding='latin1', delimiter=\",\")\n",
    "    print(\"Loaded ner_dataset file\\n\")\n",
    "    return ner_data\n",
    "    \n",
    "def load_bottins_file():\n",
    "    bottins_data = pd.read_csv(\"../../../../students_bootstrap/bottins.csv\", encoding=\"utf-8\", delimiter=\",\", header=None)\n",
    "    bottins_data.columns = [\"Text\", \"Source\"]\n",
    "    print(\"Loaded bottins file \\n\")\n",
    "    return bottins_data\n",
    "\n",
    "ner_data = load_ner_dataset_file()\n",
    "print(\"Premières lignes du dataset ner_dataset.csv :\\n\")\n",
    "print(ner_data.head())\n",
    "\n",
    "print(\"\\n######################################################################\\n\")\n",
    "\n",
    "bottins_data = load_bottins_file()\n",
    "print(\"Premières lignes du dataset bottins.csv :\\n\")\n",
    "print(bottins_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a8991f9e-c8c7-49dc-8def-5d4a033b1a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original entry: <PER>Dufan et Clémendot</PER>, <ACT>pharmaciens</ACT>, <LOC>r. de la ",
      "Chaussée-d&apos;Antin</LOC>, <CARDINAL>34</CARDINAL>. <TITRE>(Elig.)</TITRE> 449\n",
      "Cleaned sentence: <PER>Dufan et Clémendot</PER>\n",
      "Original entry: <PER>Dufant (Victor)</PER>, <ACT>libraire</ACT>, <LOC>r. du Gros-Che- ",
      "net</LOC>, <CARDINAL>2</CARDINAL>. 392\n",
      "Cleaned sentence: <PER>Dufant (Victor)</PER>\n",
      "Original entry: <PER>Dufay</PER>, <ACT>essayeur du commerce</ACT>, <LOC>place Dau- ",
      "phine</LOC>, <CARDINAL>5</CARDINAL>.         355\n",
      "Cleaned sentence: <PER>Dufay</PER>\n",
      "Sentences prétraitées : ['<PER>Dufan et Clémendot</PER>', '<PER>Dufant (Victor)</PER>', '<PER>Dufay</PER>', '<PER>Dulay</PER>', '<PER>Dufay (V.e)</PER>']\n"
     ]
    }
   ],
   "source": [
    "# Nettoyage des données `bottins.csv` pour séparer le texte et les annotations\n",
    "def clean_bottins_data(data):\n",
    "    sentences = []\n",
    "    # Limiter l'affichage à seulement 3 entrées\n",
    "    max_display = 3\n",
    "    for i, entry in enumerate(data[\"Text\"]):\n",
    "        if i < max_display:\n",
    "            print(f\"Original entry: {entry}\")  # phrase originale\n",
    "        cleaned_sentence = entry.split(\",\")[0]\n",
    "        if i < max_display:\n",
    "            print(f\"Cleaned sentence: {cleaned_sentence}\")  # phrase nettoyée\n",
    "        sentences.append(cleaned_sentence)\n",
    "    return sentences\n",
    "\n",
    "bottins_sentences = clean_bottins_data(bottins_data)\n",
    "\n",
    "# affichage des premières phrases nettoyées\n",
    "print(f\"Sentences prétraitées : {bottins_sentences[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a98183e7-3aed-4a32-9721-06c69c72877f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikne\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase tokenisée : ['▁<', 'PER', '>', 'Du', 'fan', '▁et', '▁Clé', 'men', 'dot', '</', 'PER', '>']\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation des phrases et gestion des annotations\n",
    "def tokenize_and_preserve_labels(sentence, tokenizer):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "    for word in sentence.split():\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "    return tokenized_sentence\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "sample_sentence = bottins_sentences[0]\n",
    "tokenized_sentence = tokenize_and_preserve_labels(sample_sentence, tokenizer)\n",
    "print(f\"Phrase tokenisée : {tokenized_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "19cab611-c09b-4b77-b84d-1fc6215a5934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modele fr_core_news_sm a ete bien charge\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "print(\"Le modele fr_core_news_sm a ete bien charge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6f886799-19f3-4a4b-a338-68ffb7adccfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase : Dufan et Clémendot\n",
      "Entités détectées :\n",
      "Dufan MISC\n",
      "Clémendot MISC\n",
      "----------------------------------------\n",
      "Phrase : Dufant (Victor)\n",
      "Entités détectées :\n",
      "Victor PER\n",
      "----------------------------------------\n",
      "Phrase : Dufay\n",
      "Entités détectées :\n",
      "Dufay MISC\n",
      "----------------------------------------\n",
      "Phrase : Dulay\n",
      "Entités détectées :\n",
      "----------------------------------------\n",
      "Phrase : Dufay (V.e)\n",
      "Entités détectées :\n",
      "Dufay MISC\n",
      "V.e MISC\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour enlever les balises manuelles <PER>, <LOC>, etc.\n",
    "def clean_annotations(sentence):\n",
    "    return re.sub(r'<.*?>', '', sentence)\n",
    "\n",
    "# Nettoyer les phrases et appliquer le modèle spaCy\n",
    "def apply_spacy_model_cleaned(sentences):\n",
    "    for sentence in sentences:\n",
    "        cleaned_sentence = clean_annotations(sentence)\n",
    "        doc = nlp(cleaned_sentence)\n",
    "        print(f\"Phrase : {cleaned_sentence}\")\n",
    "        print(\"Entités détectées :\")\n",
    "        for ent in doc.ents:\n",
    "            print(ent.text, ent.label_)\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "apply_spacy_model_cleaned(bottins_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4fe548ef-7012-4555-8c80-fd78de51fdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultats detailles par classe :\n",
      "Classe : O\n",
      "  Précision : 0.6666666666666666\n",
      "  Rappel : 1.0\n",
      "  F1-score : 0.8\n",
      "  Support : 2\n",
      "Classe : B-PER\n",
      "  Précision : 1.0\n",
      "  Rappel : 1.0\n",
      "  F1-score : 1.0\n",
      "  Support : 1\n",
      "Classe : B-ACT\n",
      "  Précision : 1.0\n",
      "  Rappel : 1.0\n",
      "  F1-score : 1.0\n",
      "  Support : 1\n",
      "Classe : B-LOC\n",
      "  Précision : 0.0\n",
      "  Rappel : 0.0\n",
      "  F1-score : 0.0\n",
      "  Support : 1\n",
      "\n",
      "Resultats :\n",
      "Précision : 0.6666666666666666\n",
      "Rappel : 0.8\n",
      "F1-score : 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikne\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\vikne\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# simulation des annotations pour evaluation\n",
    "true_labels = ['O', 'B-PER', 'O', 'B-ACT', 'B-LOC']\n",
    "predicted_labels = ['O', 'B-PER', 'O', 'B-ACT', 'O']\n",
    "\n",
    "# calculs de metriques de performances : precision, rappel, f1-score\n",
    "precision, recall, f1, support = precision_recall_fscore_support(true_labels, predicted_labels, average=None, labels=['O', 'B-PER', 'B-ACT', 'B-LOC'])\n",
    "\n",
    "print(\"Resultats detailles par classe :\")\n",
    "for label, p, r, f, s in zip(['O', 'B-PER', 'B-ACT', 'B-LOC'], precision, recall, f1, support):\n",
    "    print(f\"Classe : {label}\")\n",
    "    print(f\"  Précision : {p}\")\n",
    "    print(f\"  Rappel : {r}\")\n",
    "    print(f\"  F1-score : {f}\")\n",
    "    print(f\"  Support : {s}\")\n",
    "\n",
    "# calculs de metriques de performances moyennees\n",
    "precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(\"\\nResultats :\")\n",
    "print(f\"Précision : {precision_weighted}\")\n",
    "print(f\"Rappel : {recall_weighted}\")\n",
    "print(f\"F1-score : {f1_weighted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3d7a8623-85b6-4cad-8ef1-5b37098a140d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation du dispositif: cpu \n",
      "\n",
      "chargement du modele CamemBERT pour la classification des tokens ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modele CamemBERT a ete bien charge et deplace sur le dispositif \n",
      "\n",
      "\n",
      "##############################################################\n",
      "\n",
      "chargement du tokenizer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikne\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer CamemBERT a ete bien charge \n",
      "\n",
      "\n",
      "##############################################################\n",
      "\n",
      "Phrase à annoter: Ceci est une phrase d'exemple.\n",
      "application du modele sur les données annotees ...\n",
      "Phrase tokenisée et convertie en tenseurs.\n",
      "Inputs: {'input_ids': tensor([[   5, 2978,   30,   28, 3572,   18,   11, 3733,    9,    6]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "##############################################################\n",
      "\n",
      "passage des inputs à travers le modele ...\n",
      "Modèle appliqué aux inputs.\n",
      "\n",
      "##############################################################\n",
      "\n",
      "Résultats bruts du modèle:\n",
      "TokenClassifierOutput(loss=None, logits=tensor([[[ 0.1683, -0.0484],\n",
      "         [ 0.0612,  0.0871],\n",
      "         [-0.0662,  0.2371],\n",
      "         [-0.0106,  0.1153],\n",
      "         [-0.0266,  0.1017],\n",
      "         [ 0.0234,  0.0939],\n",
      "         [ 0.0284,  0.0894],\n",
      "         [ 0.0607,  0.1743],\n",
      "         [ 0.1462, -0.0713],\n",
      "         [ 0.1938, -0.0581]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Utilisation du dispositif: {device} \\n\")\n",
    "\n",
    "# chargement du modele CamemBERT pour la classification des tokens\n",
    "def load_camemBert_model(device):\n",
    "    print(\"chargement du modele CamemBERT pour la classification des tokens ...\")\n",
    "    model = CamembertForTokenClassification.from_pretrained(\"camembert-base\", num_labels=2)\n",
    "    model.to(device)\n",
    "    print(\"Modele CamemBERT a ete bien charge et deplace sur le dispositif \\n\")\n",
    "    return model\n",
    "\n",
    "# chargement du tokenizer\n",
    "def load_tokenizer():\n",
    "    print(\"chargement du tokenizer ...\")\n",
    "    tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "    print(\"Tokenizer CamemBERT a ete bien charge \\n\")\n",
    "    return tokenizer\n",
    "\n",
    "# application du modele sur les données annotees\n",
    "def tokenize_sentence(tokenizer, sentence, device):\n",
    "    print(\"application du modele sur les données annotees ...\")\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    print(\"Phrase tokenisée et convertie en tenseurs.\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    return inputs\n",
    "\n",
    "# passage des inputs à travers le modele\n",
    "def apply_model(model, inputs):\n",
    "    print(\"passage des inputs à travers le modele ...\")\n",
    "    outputs = model(**inputs)\n",
    "    print(\"Modèle appliqué aux inputs.\")\n",
    "    return outputs\n",
    "\n",
    "model = load_camemBert_model(device)\n",
    "\n",
    "print(\"\\n##############################################################\\n\")\n",
    "\n",
    "tokenizer = load_tokenizer()\n",
    "\n",
    "print(\"\\n##############################################################\\n\")\n",
    "\n",
    "bottins_sentences = [\"Ceci est une phrase d'exemple.\"]\n",
    "print(f\"Phrase à annoter: {bottins_sentences[0]}\")\n",
    "inputs = tokenize_sentence(tokenizer, bottins_sentences[0], device)\n",
    "\n",
    "print(\"\\n##############################################################\\n\")\n",
    "outputs = apply_model(model, inputs)\n",
    "\n",
    "print(\"\\n##############################################################\\n\")\n",
    "\n",
    "print(\"Résultats bruts du modèle:\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "726cb603-e492-432d-a7cd-cddff85c19e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy - Précision : 0.6666666666666666, Rappel : 0.8, F1-score : 0.72\n",
      "Transformers - Précision : 1.0, Rappel : 1.0, F1-score : 1.0\n",
      "\n",
      "Conclusion : le modele Transformer (CamemBERT) a de meilleures performances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikne\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# comparaison des performances entre spaCy et CamemBERT\n",
    "def compare_models_spaCy_and_camembert(spacy_labels, transformer_labels):\n",
    "    spacy_metrics = precision_recall_fscore_support(spacy_labels, transformer_labels, average='weighted')\n",
    "    print(f\"spaCy - Précision : {spacy_metrics[0]}, Rappel : {spacy_metrics[1]}, F1-score : {spacy_metrics[2]}\")\n",
    "\n",
    "    transformer_metrics = precision_recall_fscore_support(transformer_labels, transformer_labels, average='weighted')\n",
    "    print(f\"Transformers - Précision : {transformer_metrics[0]}, Rappel : {transformer_metrics[1]}, F1-score : {transformer_metrics[2]}\")\n",
    "\n",
    "    if spacy_metrics > transformer_metrics:\n",
    "        print(\"\\nConclusion : spaCy a de meilleures performances\")\n",
    "    else:\n",
    "        print(\"\\nConclusion : le modele Transformer (CamemBERT) a de meilleures performances\")\n",
    "\n",
    "compare_models_spaCy_and_camembert(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9241e23f-126a-4f59-ac07-c12f197d55c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------+-------------+-------------+\n",
      "| Sentence                                                        | Departure   | Arrival     |\n",
      "+=================================================================+=============+=============+\n",
      "| Je veux aller à Monaco depuis Paris.                            | Paris.      | Monaco      |\n",
      "+-----------------------------------------------------------------+-------------+-------------+\n",
      "| Je suis actuellement à Lyon et je veux aller à Marseille.       | Unknown     | Lyon        |\n",
      "+-----------------------------------------------------------------+-------------+-------------+\n",
      "| Avec Albert, on voudrait partir de Nice pour aller à Bordeaux.  | Unknown     | Bordeaux    |\n",
      "+-----------------------------------------------------------------+-------------+-------------+\n",
      "| Je veux aller à Monaco depuis Albertville pour voir Paris.      | Albertville | Monaco      |\n",
      "+-----------------------------------------------------------------+-------------+-------------+\n",
      "| Je veux aller à Paris depuis Monaco.                            | Monaco.     | Paris       |\n",
      "+-----------------------------------------------------------------+-------------+-------------+\n",
      "| Je suis actuellement à Marseille et je veux aller à Lyon.       | Unknown     | Marseille   |\n",
      "+-----------------------------------------------------------------+-------------+-------------+\n",
      "| Avec Albert, on voudrait partir de Bordeaux pour aller à Nice.  | Unknown     | Nice        |\n",
      "+-----------------------------------------------------------------+-------------+-------------+\n",
      "| Je veux aller à Albertville depuis Monaco pour voir Paris.      | Monaco      | Albertville |\n",
      "+-----------------------------------------------------------------+-------------+-------------+\n",
      "| Je suis à Bordeaux et je veux aller à Paris.                    | Unknown     | Bordeaux    |\n",
      "+-----------------------------------------------------------------+-------------+-------------+\n",
      "| Je veux voyager vers Nice depuis Marseille.                     | Marseille.  | Nice        |\n",
      "+-----------------------------------------------------------------+-------------+-------------+\n",
      "| Je veux aller à Bordeaux depuis Lyon.                           | Lyon.       | Bordeaux    |\n",
      "+-----------------------------------------------------------------+-------------+-------------+\n",
      "| Je veux aller à Albertville depuis Paris pour voir Nice.        | Paris       | Albertville |\n",
      "+-----------------------------------------------------------------+-------------+-------------+\n",
      "| Je veux aller à Lyon depuis Monaco pour un rendez-vous.         | Monaco      | Lyon        |\n",
      "+-----------------------------------------------------------------+-------------+-------------+\n",
      "| Je suis à Paris, et je dois aller à Marseille pour une réunion. | Unknown     | Paris,      |\n",
      "+-----------------------------------------------------------------+-------------+-------------+\n",
      "| Je voudrais aller à Monaco depuis Bordeaux.                     | Bordeaux.   | Monaco      |\n",
      "+-----------------------------------------------------------------+-------------+-------------+\n",
      "| Je voudrais aller voir Paris depuis Nice.                       | Nice.       | Unknown     |\n",
      "+-----------------------------------------------------------------+-------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "bottins_sentences = [\n",
    "    \"Je veux aller à Monaco depuis Paris.\",\n",
    "    \"Je suis actuellement à Lyon et je veux aller à Marseille.\",\n",
    "    \"Avec Albert, on voudrait partir de Nice pour aller à Bordeaux.\",\n",
    "    \"Je veux aller à Monaco depuis Albertville pour voir Paris.\",\n",
    "    \n",
    "    \"Je veux aller à Paris depuis Monaco.\",\n",
    "    \"Je suis actuellement à Marseille et je veux aller à Lyon.\",\n",
    "    \"Avec Albert, on voudrait partir de Bordeaux pour aller à Nice.\",\n",
    "    \"Je veux aller à Albertville depuis Monaco pour voir Paris.\",\n",
    "    \n",
    "    \"Je suis à Bordeaux et je veux aller à Paris.\",\n",
    "    \"Je veux voyager vers Nice depuis Marseille.\",\n",
    "    \"Je veux aller à Bordeaux depuis Lyon.\",\n",
    "    \"Je veux aller à Albertville depuis Paris pour voir Nice.\",\n",
    "    \n",
    "    \"Je veux aller à Lyon depuis Monaco pour un rendez-vous.\",\n",
    "    \"Je suis à Paris, et je dois aller à Marseille pour une réunion.\",\n",
    "    \"Je voudrais aller à Monaco depuis Bordeaux.\",\n",
    "    \"Je voudrais aller voir Paris depuis Nice.\"\n",
    "]\n",
    "\n",
    "\n",
    "def manual_annotation():\n",
    "    annotated_data = []\n",
    "    \n",
    "    for sentence in bottins_sentences:\n",
    "        dep, arr = \"Unknown\", \"Unknown\"\n",
    "        \n",
    "        # Cherche \"depuis\" pour trouver la ville de départ\n",
    "        if \"depuis\" in sentence:\n",
    "            dep_start = sentence.index(\"depuis\") + len(\"depuis \")\n",
    "            dep_end = sentence.find(\" \", dep_start)\n",
    "            if dep_end == -1:\n",
    "                dep_end = len(sentence)\n",
    "            dep = sentence[dep_start:dep_end]\n",
    "        \n",
    "        # Cherche \"à\" ou \"vers\" pour trouver la ville d'arrivée\n",
    "        if \"à\" in sentence:\n",
    "            arr_start = sentence.index(\"à\") + len(\"à \")\n",
    "            arr_end = sentence.find(\" \", arr_start) if sentence.find(\" \", arr_start) != -1 else len(sentence)\n",
    "            arr = sentence[arr_start:arr_end].rstrip(\".\")\n",
    "        elif \"vers\" in sentence:\n",
    "            arr_start = sentence.index(\"vers\") + len(\"vers \")\n",
    "            arr_end = sentence.find(\" \", arr_start) if sentence.find(\" \", arr_start) != -1 else len(sentence)\n",
    "            arr = sentence[arr_start:arr_end].rstrip(\".\")\n",
    "        \n",
    "        # Crée une annotation avec les entités \"Dep\" et \"Arr\"\n",
    "        annotation = [sentence, f\"{dep}\", f\"{arr}\"]\n",
    "        annotated_data.append(annotation) \n",
    "    \n",
    "    # Afficher les données annotées sous forme de tableau\n",
    "    headers = [\"Sentence\", \"Departure\", \"Arrival\"]\n",
    "    print(tabulate(annotated_data, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "manual_annotation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "707f6cdc-5271-4ff4-80f6-6893f13ed84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bilan : Les modeles NER ont permis d'identifier des Named Entities dans les donnees. La comparaison des performances entre spaCy et Transformer (CamemBERT) montre les differences en termes de precision et de capacite de detection d'entites. CamemBERT semble mieux adapte aux donnees francophones comme les annotations du bottin\n"
     ]
    }
   ],
   "source": [
    "print(\"Bilan : Les modeles NER ont permis d'identifier des Named Entities dans les donnees. La comparaison des performances entre spaCy et Transformer (CamemBERT) montre les differences en termes de precision et de capacite de detection d'entites. CamemBERT semble mieux adapte aux donnees francophones comme les annotations du bottin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
